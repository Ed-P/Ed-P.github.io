<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/star180.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/star32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/star16.ico">
  <link rel="mask-icon" href="/images/star.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.loli.net/css?family=Noto+Serif+SC:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"edwardpei.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"ÊêúÁ¥¢...","empty":"Ê≤°ÊúâÊâæÂà∞‰ªª‰ΩïÊêúÁ¥¢ÁªìÊûúÔºö${query}","hits_time":"ÊâæÂà∞ ${hits} ‰∏™ÊêúÁ¥¢ÁªìÊûúÔºàÁî®Êó∂ ${time} ÊØ´ÁßíÔºâ","hits":"ÊâæÂà∞ ${hits} ‰∏™ÊêúÁ¥¢ÁªìÊûú"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"manual","top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="An introduction and summary of quantization methods.">
<meta property="og:type" content="article">
<meta property="og:title" content="Quantization">
<meta property="og:url" content="https://edwardpei.com/2021/12/12/Quantization/index.html">
<meta property="og:site_name" content="ÊµÅÂçÉËøáÂÆ¢-Ed-P">
<meta property="og:description" content="An introduction and summary of quantization methods.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/nanotes2021students.pdf.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/nanotes2021students1.pdf.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213104448578.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213110511521.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213112908737.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213112930698.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213113033704.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211215141117555.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211213174837627.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211214144955255.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/c5d18de467f385b522356fb0fc30d88f.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/bdc1f96f691ba36f76e9f9d2bf500c4b.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/028f7224f92f74c2161c21585811786b.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/ffac2798a6530bf4599c97747b8ba29a.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/fba76a8a630f943cde687b2512377613.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211214162441864.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211214162526049.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/ff57f0dd7bc7e0b72311341a75276378.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211214181305441.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211214182006243.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211215152038288.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211215152121988.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211228113252203.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211228141327650.png">
<meta property="og:image" content="https://edwardpei.com/2021/12/12/Quantization/image-20211228142202635.png">
<meta property="article:published_time" content="2021-12-12T13:22:00.000Z">
<meta property="article:modified_time" content="2021-12-28T12:41:34.000Z">
<meta property="article:author" content="Edward PEI">
<meta property="article:tag" content="Efficient_Neural_Network">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://edwardpei.com/2021/12/12/Quantization/nanotes2021students.pdf.png">


<link rel="canonical" href="https://edwardpei.com/2021/12/12/Quantization/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://edwardpei.com/2021/12/12/Quantization/","path":"2021/12/12/Quantization/","title":"Quantization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Quantization | ÊµÅÂçÉËøáÂÆ¢-Ed-P</title>
  



<link rel="stylesheet" href="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.css"><style>
#needsharebutton-postbottom {
  cursor: pointer;
  height: 26px;
  margin-top: 10px;
  position: relative;
}
#needsharebutton-postbottom .btn {
  border: 1px solid $btn-default-border-color;
  border-radius: 3px;
  display: initial;
  padding: 1px 4px;
}
</style>
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="ÂàáÊç¢ÂØºËà™Ê†è" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">ÊµÅÂçÉËøáÂÆ¢-Ed-P</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>È¶ñÈ°µ</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>ÂÖ≥‰∫é</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Ê†áÁ≠æ</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>ÂàÜÁ±ª</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>ÂΩíÊ°£</a></li>
        <li class="menu-item menu-item-messageboard"><a href="/messageboard/" rel="section"><i class="fa fa-comment fa-fw"></i>ÁïôË®ÄÊùø</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>ÊêúÁ¥¢
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="ÊêúÁ¥¢..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          ÊñáÁ´†ÁõÆÂΩï
        </li>
        <li class="sidebar-nav-overview">
          Á´ôÁÇπÊ¶ÇËßà
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Preliminary"><span class="nav-number">1.</span> <span class="nav-text">Preliminary</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Intro"><span class="nav-number">1.1.</span> <span class="nav-text">Intro.</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binary-system"><span class="nav-number">1.2.</span> <span class="nav-text">Binary system</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Normalized-scientific-notation-and-floating-point-representation"><span class="nav-number">1.2.1.</span> <span class="nav-text">Normalized scientific notation and floating-point  representation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Integer-in-binary"><span class="nav-number">1.2.2.</span> <span class="nav-text">Integer in binary</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-quantization"><span class="nav-number">1.3.</span> <span class="nav-text">Why quantization?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference-and-Training"><span class="nav-number">1.4.</span> <span class="nav-text">Inference and Training</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Basic-Definition"><span class="nav-number">2.</span> <span class="nav-text">Basic Definition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Asymmetric-and-symmetric-quantization"><span class="nav-number">2.1.</span> <span class="nav-text">Asymmetric and symmetric quantization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Asymmetric-Affine-Quantization"><span class="nav-number">2.1.1.</span> <span class="nav-text">Asymmetric (Affine) Quantization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Symmetric-Scale-Quantization"><span class="nav-number">2.1.2.</span> <span class="nav-text">Symmetric (Scale) Quantization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Comparison"><span class="nav-number">2.1.3.</span> <span class="nav-text">Comparison</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Calibration"><span class="nav-number">2.2.</span> <span class="nav-text">Calibration</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Min-Max"><span class="nav-number">2.2.1.</span> <span class="nav-text">Min&#x2F;Max</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Clipping"><span class="nav-number">2.2.2.</span> <span class="nav-text">Clipping</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Exponential-moving-average-EMA"><span class="nav-number">2.2.3.</span> <span class="nav-text">Exponential moving average (EMA)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Kullback-Leibler-divergence-KL"><span class="nav-number">2.2.4.</span> <span class="nav-text">Kullback-Leibler divergence (KL)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Percentile"><span class="nav-number">2.2.5.</span> <span class="nav-text">Percentile</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dequantization"><span class="nav-number">2.3.</span> <span class="nav-text">Dequantization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantization-Granularity"><span class="nav-number">2.4.</span> <span class="nav-text">Quantization Granularity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Post-training-Quantization-PTQ"><span class="nav-number">3.</span> <span class="nav-text">Post-training-Quantization (PTQ)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Quantization-Aware-Training-QAT"><span class="nav-number">4.</span> <span class="nav-text">Quantization-Aware-Training (QAT)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fake-Quantization-simulated-quantization"><span class="nav-number">4.1.</span> <span class="nav-text">Fake Quantization (simulated quantization)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Straight-Through-Estimator-STE"><span class="nav-number">4.2.</span> <span class="nav-text">Straight-Through Estimator (STE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Quantization-Parameters"><span class="nav-number">4.3.</span> <span class="nav-text">Learning Quantization Parameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-quantization-full-training"><span class="nav-number">5.</span> <span class="nav-text">Gradient quantization (full training)</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Edward PEI"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Edward PEI</p>
  <div class="site-description" itemprop="description">Fortune favours the brave</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">Êó•Âøó</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">ÂàÜÁ±ª</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">Ê†áÁ≠æ</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="ËøîÂõûÈ°∂ÈÉ®">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://edwardpei.com/2021/12/12/Quantization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Edward PEI">
      <meta itemprop="description" content="Fortune favours the brave">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ÊµÅÂçÉËøáÂÆ¢-Ed-P">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Quantization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">ÂèëË°®‰∫é</span>

      <time title="ÂàõÂª∫Êó∂Èó¥Ôºö2021-12-12 21:22:00" itemprop="dateCreated datePublished" datetime="2021-12-12T21:22:00+08:00">2021-12-12</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Êõ¥Êñ∞‰∫é</span>
        <time title="‰øÆÊîπÊó∂Èó¥Ôºö2021-12-28 20:41:34" itemprop="dateModified" datetime="2021-12-28T20:41:34+08:00">2021-12-28</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">ÂàÜÁ±ª‰∫é</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Knowledge/" itemprop="url" rel="index"><span itemprop="name">Knowledge</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="Êú¨ÊñáÂ≠óÊï∞">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Êú¨ÊñáÂ≠óÊï∞Ôºö</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="ÈòÖËØªÊó∂Èïø">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">ÈòÖËØªÊó∂Èïø &asymp;</span>
      <span>14 ÂàÜÈíü</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>An introduction and summary of quantization methods. </p>
<span id="more"></span>

<p>I will rewrite it by LaTeX if necessary for better reading.</p>
<h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><h3 id="Intro"><a href="#Intro" class="headerlink" title="Intro."></a>Intro.</h3><p>What does quantization mean? Efficient? How can it work?</p>
<p>In Wikipedia, they define quantization as: ‚ÄúQuantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements.‚Äù  It‚Äôs trivial that we need precise numbers during computation to obtain precise results. A precise number is always the values in the real number set, which is actually a large set. </p>
<p>However, using more precise numbers means that you needs more memory to store it (it‚Äôs the same in the real world, imaging that you are struggling to remember some specific things, such as <em>œÄ</em> ). So, reducing the precision to remember something easily is natural. And that is what quantization does. For computer, it is not different, and it may be more clear to see why. The goal of quantization is to reduce the precision of the parameters (weights, bias) and the intermediate activations to low-precision.</p>
<h3 id="Binary-system"><a href="#Binary-system" class="headerlink" title="Binary system"></a>Binary system</h3><p>We use decimal system in our daily life, with 10 numbers and can write any number in powers of 10:</p>
<p><img src="nanotes2021students.pdf.png" alt="nanotes2021students.pdf"></p>
<p>Computers work internally in the binary system with only two digits 0 and 1, the real number 9.90625 can be written as:</p>
<p><img src="nanotes2021students1.pdf.png" alt="nanotes2021students1.pdf"></p>
<h4 id="Normalized-scientific-notation-and-floating-point-representation"><a href="#Normalized-scientific-notation-and-floating-point-representation" class="headerlink" title="Normalized scientific notation and floating-point  representation"></a>Normalized scientific notation and floating-point  representation</h4><p>In decimal system, we express any real number in normalized scientific notation:</p>
<p><img src="image-20211213104448578.png" alt="image-20211213104448578"></p>
<p>where r is a real number in the range 0.1 &lt;= r &lt; 1 and n is an integer.</p>
<p>The real number in the form 0.0025 is called a fixed-point number, and its floating-point representation is 0.259*10^{-2} (just the normalized scientific notation). </p>
<p>Since computers can only operate using real numbers expressed in a <strong>fixed</strong> number of digits (depending on their word length), by using the floating-point representation we can express the numbers in a larger range. For example, with a fixed 8 digits, one can only express the real number in fixed-point form from 0.0000001 (10^-7) to 9.9999999 (‚âà10). However, in the floating-point representation the range is from 10^-99 to 10^99 if we use 2 of the digits to represent a power if 10. But at what cost? The accuracy of the numbers of floating-point representation will be relatively lower (it is similar to our main topic quantization).</p>
<p>In binary system, we have similar representation:</p>
<p><img src="image-20211213110511521.png" alt="image-20211213110511521"></p>
<p>where 1*2^-1 &lt;= q &lt; 1( 1/2,1) and m is an integer. q is called the mantissa and m is called the exponent and they are both in the binary form. Actually, m decides the movement of the binary point in q. For example, if m equals to 4 (in binary form 100), we may shift  the binary point 4 places to the right.</p>
<p>Since the first nonzero bit must be 1 in the mantissa, we can express q as (1.f)_2 and hence the first bit does need to store.</p>
<p>The accuracy of a number represented by a computer depends on the word length of the computer, such as 32 bits, 64 bits. The word length is divided into three section in the floating-point representation (here use 32 bits as example): 1. one bit for the sign if q; 2. 8 bits for m (there also contains 1 bit for m‚Äôs sign); 3. 23 bits for q.</p>
<p>(FP16: 1: sign, 5: m, 10: q.) </p>
<p>(FP32: Single-precision floating-point format; FP16: Half-precision floating-point format)</p>
<p>It has the following form:</p>
<p><img src="image-20211213112908737.png" alt="image-20211213112908737"></p>
<p>where</p>
<p><img src="image-20211213112930698.png" alt="image-20211213112930698"></p>
<p>and</p>
<p><img src="image-20211213113033704.png" alt="image-20211213113033704"></p>
<p>Noting that m has 7 digits in binary, so the largest m is 127. Hence a computer with word length 32 bits can handle numbers as small as (1.f * 2^-m) and as large as (1.f * 2^m). (Can not be used directly, the range of m has some trick, you can see in the next section)</p>
<h4 id="Integer-in-binary"><a href="#Integer-in-binary" class="headerlink" title="Integer in binary"></a>Integer in binary</h4><p><strong>int8</strong>: using 1 byte (8 bits) to express the integer with sign.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">The first bit is used to express the sign, 1-negative; 0-positive.</span><br><span class="line"></span><br><span class="line">The largest value in the binary form:</span><br><span class="line">0 1 1 1 1 1 1 1</span><br><span class="line"></span><br><span class="line">In decimal form:</span><br><span class="line">0       1       1       1       1       1       1       1</span><br><span class="line">0*2^7 + 1*2^6 + 1*2^5 + 1*2^4 + 1*2^3 + 1*2^2 + 1*2^1 + 1*2^0</span><br><span class="line">0     + 64    + 32    + 16    + 8     + 4     + 2     + 1</span><br><span class="line">= 127</span><br><span class="line"></span><br><span class="line">The smallest value:</span><br><span class="line">1 0 0 0 0 0 0 0</span><br><span class="line"></span><br><span class="line">In decimal form:</span><br><span class="line"> 1       0       0       0       0       0       0       0</span><br><span class="line">-1*2^7 + 0*2^6 + 0*2^5 + 0*2^4 + 0*2^3 + 0*2^2 + 0*2^1 + 0*2^0</span><br><span class="line">-128   + 0     + 0     + 0     + 0     + 0     + 0     + 0</span><br><span class="line"> = -128</span><br></pre></td></tr></table></figure>

<p><strong>uint8</strong>: using 1 byte (8 bits) to express the integer without sign.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2^8-1 = 256-1 = 255</span><br><span class="line">since 0 occupies one position.</span><br><span class="line">0~255</span><br></pre></td></tr></table></figure>

<h3 id="Why-quantization"><a href="#Why-quantization" class="headerlink" title="Why quantization?"></a>Why quantization?</h3><p>So, quantization is just to transfer the values (weights, bias, activations) stored in the floating-point form in 32FP to the data type int8/unit8. (8/4/2/1 bit quantization, 8 bit is the most common one.) </p>
<p>When we apply quantization into a deep learning model</p>
<ol>
<li>More efficient computation. It is more efficient for many processors to handle integer calculation than corresponding floating -point calculations (Why?). On CPU, the latency of floating-point calculation command is averagely longer than the corresponding integer calculation command. </li>
<li>Reduce memory footprint and storage footprint. Reduce the time cost in memory access. For FP32, each weight/bias need 32-bit storage, and it will become 8-bit after quantization. </li>
<li>(Reduce the energy consumption)</li>
</ol>
<p><img src="image-20211215141117555.png" alt="image-20211215141117555"></p>
<h3 id="Inference-and-Training"><a href="#Inference-and-Training" class="headerlink" title="Inference and Training"></a>Inference and Training</h3><p>Although quantization can achieve good result in reducing the network computation, we still need complex and high-accuracy model in training to capture the slight gradient change. It‚Äôs a trade-off between the precision and efficiency. </p>
<p>Therefore, most of the time, we apply quantization on the trained neural network, which has become convergence. Then the precision will be maintained in best possible after quantization. This is the quantization on inference.</p>
<p>When applying quantization in running inference, we have PTQ and QAT approaches, which will be discussed later. </p>
<p>In fact, it‚Äôs not like we don‚Äôt use quantization in training of neural networks. QAT uses some techniques to recover the accuracy of models by re-training the networks with quantized parameters.</p>
<p>There also are algorithms focus on training the models on quantization from the very beginning. It seems to pay attention to better gradient quantization. </p>
<h2 id="Basic-Definition"><a href="#Basic-Definition" class="headerlink" title="Basic Definition"></a>Basic Definition</h2><p>The process of quantization is actually a rounding or truncation problem. There is a general function that maps real values in floating point to a lower precision range:</p>
<p><img src="image-20211213174837627.png" alt="image-20211213174837627"></p>
<p>where x is a real value input (weight, bias, activation), q is the value after quantization, s is called scaling factor, z is called the zero point. s may be variable in different methods, which will be discussed later(calibration). z can be used to adjust the true zero in some asymmetrical quantization approaches.</p>
<p>(activation is the output of the activation function of each layer)</p>
<p>For the rounding function, it is always the same as the round function in python, that is four dropping and five filling in Chinese **(**not exactly, the first reason is that the rounding function varies with the version of python: py2: ‚ÄúValues are rounded to the closest multiple of 10 to the power minus n digits; if two multiples are equally close, rounding is done away from 0.‚Äù py3: ‚Äúvalues are rounded to the closest multiple of 10 to the power minus n digits; if two multiples are equally close, rounding is done toward the even choice.‚Äù The second reason is that the rounding is influenced by the rounding errors of computer because of the world length limitation. Hence, you may use the functions in math or decimal modules of python. <strong>)</strong>. Sometimes researchers use the int() function in python, that is remaining the integer part regardless of the value after the decimal point. </p>
<p>Noting that here z is set inside the rounding function, but there also are some algorithms set z outside the rounding function. They are different in the searching domain of  z, the former is in real numbers domain and the latter is in the lower precision domain. The consequence is that (you can prove it by using mathematics by yourself) the domain of real values that will be quantized into zero-point will be different (yes, it is a multiple to 1 mapping): If you set it before rounding, then you can choose a specific real number and the values around it (the range depends on s) to be quantized into 0. If you set it after rounding, the you can choose a value in the lower precision domain, for example a integer in int8, then actually we are finding values that will be rounded to z ( it‚Äôs just 0, since 0 with the values around it satisfies coincidently). I should emphasize that they are different! Since the rounding function is not linear, so you can‚Äôt do the operation like h(a+b)=h(a)+h(b). Note again that, when z is 0, these two ways are the same.</p>
<h3 id="Asymmetric-and-symmetric-quantization"><a href="#Asymmetric-and-symmetric-quantization" class="headerlink" title="Asymmetric and symmetric quantization"></a>Asymmetric and symmetric quantization</h3><h4 id="Asymmetric-Affine-Quantization"><a href="#Asymmetric-Affine-Quantization" class="headerlink" title="Asymmetric (Affine) Quantization"></a>Asymmetric (Affine) Quantization</h4><p>For convenience, let‚Äôs first define s as:</p>
<p><img src="image-20211214144955255.png" alt="image-20211214144955255"></p>
<p>We can use a widely used calibration method Min/Max (discuss later), which set the clip range as [Œ≤, Œ±] = [Min_x, Max_X] and is always used in the asymmetric or symmetric quantization. Max_x and min_x is the maximum and minimum bounds of the quantizing real values (weights, bias, activation), b is the quantization bit width. This design is to make the quantized values is within the range of the lower precision:</p>
<p><img src="c5d18de467f385b522356fb0fc30d88f.png" alt="Image"></p>
<p>With Min/Max, the asymmetric quantization formula is as follows:</p>
<p><img src="bdc1f96f691ba36f76e9f9d2bf500c4b.png" alt="Image"></p>
<p>Here, the minimum bound (is negative) is chosen to corresponding to the zero-point (-zpx actually). We can adjust the choice to change the range of the lower precision. It is called asymmetric because the range on the both sides of 0 in the real set is different.</p>
<h4 id="Symmetric-Scale-Quantization"><a href="#Symmetric-Scale-Quantization" class="headerlink" title="Symmetric (Scale) Quantization"></a>Symmetric (Scale) Quantization</h4><p>If we set the zero-point as 0, then 0 and its neighbors in the real number set are maps to 0 in the lower precision domain. When we set the clip range are the same on both sides, i.e. max(|x|), then the input range and integer range are symmetric around zero and hence it is called symmetric quantization (Sometimes we don‚Äôt require the input range to be symmetric):</p>
<p><img src="028f7224f92f74c2161c21585811786b.png" alt="Image"></p>
<p>Since we need 0 maps to 0, for 8-bit quantization the integer range should be [-127, 127], deleting -128 to satisfy symmetry. Noting that it may be insignificant for 8-bit quantization since there are 256 representable values, but for lower bit quantization it should be considered signigicantly.</p>
<p>There is no zero-point in symmetric quantization (since it‚Äôs 0), the formula is simpler:</p>
<p><img src="ffac2798a6530bf4599c97747b8ba29a.png" alt="Image"></p>
<h4 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h4><p>Since there is no zero-point in symmetric quantization, which can lead to reduction in computational cost during inference.</p>
<p>However,  when the input range is not symmetric , such as [-20,1000], symmetric quantization will be inefficient since lots of bits in int8 are not used.</p>
<p>So, we may choose them according to the data distribution of the input. If it‚Äôs a approximately normal distribution around 0, using symmetric quantization is better. If it is activations after ReLU, it is obviously that using asymmetric quantization is better.</p>
<h3 id="Calibration"><a href="#Calibration" class="headerlink" title="Calibration"></a>Calibration</h3><p>Both in <strong>dynamic quantization and static quantization</strong> (In PTQ), we need to determine the clipping range of the parameters or activations. The following is the methods to define how to determine it.</p>
<h4 id="Min-Max"><a href="#Min-Max" class="headerlink" title="Min/Max"></a>Min/Max</h4><p>Min/Max is popular as we have introduced since it is simple. However,  it is susceptible to outlier data in activations, since there will be a waste for some bits position. </p>
<p>In static quantization (need a series of calibration inputs to do pre-calculation), the formula is as follows:</p>
<p><img src="fba76a8a630f943cde687b2512377613.png" alt="Image"></p>
<p>where X is one local sample, x is the global.</p>
<p>When we use calibration methods except Min/Max, the process will always be called saturated quantization contrast with Min/Max (unsaturated methods).</p>
<h4 id="Clipping"><a href="#Clipping" class="headerlink" title="Clipping"></a>Clipping</h4><p>We can do clipping on the input range before scaling to solve the problem of outlier:</p>
<p><img src="image-20211214162441864.png" alt="image-20211214162441864"></p>
<p>the formula of clip function may be:</p>
<p><img src="image-20211214162526049.png" alt="image-20211214162526049"></p>
<p>Some of the following methods are focus on how to find the clipping bounds.</p>
<h4 id="Exponential-moving-average-EMA"><a href="#Exponential-moving-average-EMA" class="headerlink" title="Exponential moving average (EMA)"></a>Exponential moving average (EMA)</h4><p>When clipping activations, sometimes we will use EMA (or MovingAverageMinMax):</p>
<p><img src="ff57f0dd7bc7e0b72311341a75276378.png" alt="Image"></p>
<p>where c is set as 0.01 by default. We can see that it tends to keep the history result and fine-turning according to local sample.</p>
<h4 id="Kullback-Leibler-divergence-KL"><a href="#Kullback-Leibler-divergence-KL" class="headerlink" title="Kullback-Leibler divergence (KL)"></a>Kullback-Leibler divergence (KL)</h4><p>KL-divergence, or relative entropy or information divergence  is a measure to show the relative entropy (how different) of probability distributions between two sets, which are defined on the same probability space.</p>
<p>We think that quantization will cause less information loss from the original data if the data distribution of quantized data is more similar to the original data. Such information loss can be calculated by KL-divergence (The default method used by TensorRT, a PTQ method).</p>
<p>For discrete probability distribution, P is the original data distribution that falls in several bins and Q is the quantized data histogram, the KL-divergence is:</p>
<p> <img src="image-20211214181305441.png" alt="image-20211214181305441"></p>
<p>In TensorRT, KL-divergence is used to optimize threshold selection (Since there is always a trade-off between range and precision of the int8 representation(for activations outliars)), the procedure is as follows:<br><img src="image-20211214182006243.png" alt="image-20211214182006243"></p>
<h4 id="Percentile"><a href="#Percentile" class="headerlink" title="Percentile"></a>Percentile</h4><p>Set the range to a percentile of the distribution of positive/negative values(when symmetric, absolute values) seen during calibration. For example, when 99% calibration, 1% values will be clipped.</p>
<h3 id="Dequantization"><a href="#Dequantization" class="headerlink" title="Dequantization"></a>Dequantization</h3><p>When applying QAT, the fake quantization (or simulated quantization) operations are always used, where the dequantization operation will be used.</p>
<p>In asymmetric quantization, the corresponding dequantize function is:</p>
<p><img src="image-20211215152038288.png" alt="image-20211215152038288"></p>
<p>In symmetric quantization:</p>
<p><img src="image-20211215152121988.png" alt="image-20211215152121988"></p>
<h3 id="Quantization-Granularity"><a href="#Quantization-Granularity" class="headerlink" title="Quantization Granularity"></a>Quantization Granularity</h3><p>We know that the clipping range is determined based on the input range (real numbers). The input range is determined by the input data. It is important to set where to collect the input data, or the choice for sharing the quantization parameters.</p>
<p>Tensor-wise, or layer-wise. The clipping range is determined by considering all the weights or activations in the convolutional filters of a layer.</p>
<p>Channel-wise, considering values in each channel, independent of other channels. For weights, it is kernel-wise or filter-wise.</p>
<p>We know that different channel of the features may contain different information, so the range of each convolutional filter can vary a lot. Hence, channel-wise quantization is suitable and is currently the standard method.</p>
<h2 id="Post-training-Quantization-PTQ"><a href="#Post-training-Quantization-PTQ" class="headerlink" title="Post-training-Quantization (PTQ)"></a>Post-training-Quantization (PTQ)</h2><p>With a pre-trained FP32 neural network, Post-training quantization (PTQ) algorithms convert it into a fixed-point network. This process doesn‚Äôt need training (hence no need to care about backpropagation) and can be data-free or require a series of calibration inputs.</p>
<p>How to quantize parameters depends on the types of parameters. For weight, this range can be computed statically or pre-calculated. Since weights are almost convergent during the training of floating-point model, so their range will not vary so much. However, since the input of the model is different every time, the activations in the network may have different range and it is very likely to have outlier. So we will focus on activations to do calibration (in Pytorch).</p>
<p>Dynamic quantization: the range is dynamically calculated for each activation map during runtime. We can use some real-time computation (Min/Max, percentile, etc.). It will achieve high accuracy but need more computational cost.</p>
<p>Static quantization: clipping range is pre-calculated by running a series of calibration inputs.</p>
<p>Note that there are some slight differences in the quantization on Pytorch and TensorFlow. In Dynamic quantization, the quantization of activations on Pytorch is dynamic and should be done, while on TensorFlow activations could not be quantized. When you choose to quantize activations, the computation between weights and activations will be done on Int8. When you choose not, the weights will be converted back to floating-point in computation. The static quantization in TensorFlow is called integer quantization.</p>
<h2 id="Quantization-Aware-Training-QAT"><a href="#Quantization-Aware-Training-QAT" class="headerlink" title="Quantization-Aware-Training (QAT)"></a>Quantization-Aware-Training (QAT)</h2><p>When we use PTQ, we always have accuracy loss, especially in low-precision quantization. Therefore we need some techniques to help recover the accuracy, and quantization-aware-training (QAT) is one of them. </p>
<p>QAT refers to inserting some quantization operations into the neural network and then retraining or fine-turning the network. This models the quantization noise in training to allow the network to adapt to the quantized weights and activations (but I think this may not have solid theoretical basis).</p>
<h3 id="Fake-Quantization-simulated-quantization"><a href="#Fake-Quantization-simulated-quantization" class="headerlink" title="Fake Quantization (simulated quantization)"></a>Fake Quantization (simulated quantization)</h3><p>The most common approach of QAT is fake quantization, or simulated quantization, which is a combination of quantize and dequantize operation that produces an approximation of the parameters:</p>
<p><img src="image-20211228113252203.png" alt="image-20211228113252203"></p>
<p>where x and x_hat are both floating-point values. The reason to apply fake quantization is to simulate the effects of quantization.</p>
<h3 id="Straight-Through-Estimator-STE"><a href="#Straight-Through-Estimator-STE" class="headerlink" title="Straight-Through Estimator (STE)"></a>Straight-Through Estimator (STE)</h3><p>In order to achieve such purpose, we need to retraining the simulated network. However, the gradient of the rounding operation is either 0 or undefined everywhere (step function) and then can‚Äôt be used as back-propagation. Hence, we need to design some techniques to solve such issue. Straight-through estimator is one of the solutions by approximating the gradient of the rounding operator as 1:</p>
<p><img src="image-20211228141327650.png" alt="image-20211228141327650"></p>
<p>By using this, we we can calculate the gradient of the quantize and dequantize operation w.r.t the input:</p>
<p><img src="image-20211228142202635.png" alt="image-20211228142202635"></p>
<p>where [q_min,q_max] is the clipping range (the s in this picture equals to the 1/s of previous).</p>
<p>Note that we actually skip the quantize and dequantize operation in backward pass since STE approximates the gradient as 1 when the input is in the clipping range.</p>
<h3 id="Learning-Quantization-Parameters"><a href="#Learning-Quantization-Parameters" class="headerlink" title="Learning Quantization Parameters"></a>Learning Quantization Parameters</h3><p>As we have indicated, the quantization parameters (clipping range) depend on the inference by using a set of calibration. But we also can learn the quantization parameters along with the model weights. PACT‚Ä¶‚Ä¶</p>
<p><strong>!!!!!</strong> <strong>After training, the network is transformed by quantization again (i.e. using static quantization) to the low-precision network.</strong></p>
<h2 id="Gradient-quantization-full-training"><a href="#Gradient-quantization-full-training" class="headerlink" title="Gradient quantization (full training)"></a>Gradient quantization (full training)</h2>
    </div>

    
    
    

    <footer class="post-footer"><div class="post-widgets">
      <div id="needsharebutton-postbottom">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    </div>
          <div class="reward-container">
  <div>‰ªäÂ§©ÂêÉÊñπ‰æøÈù¢ÔºåÁ∫¢ÁÉßËøòÊòØÊ∏ÖËí∏</div>
  <button>
    ËµûËµè
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="Edward PEI ÂæÆ‰ø°">
        <span>ÂæÆ‰ø°</span>
      </div>

  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Efficient-Neural-Network/" rel="tag"># Efficient_Neural_Network</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/11/25/Mean-Average-Precision%EF%BC%88MAP%EF%BC%89/" rel="prev" title="Mean Average PrecisionÔºàMAP)">
                  <i class="fa fa-chevron-left"></i> Mean Average PrecisionÔºàMAP)
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/12/Knowledge-Distillation/" rel="next" title="Knowledge_Distillation">
                  Knowledge_Distillation <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments">
  <script src="https://utteranc.es/client.js" repo="Ed-P/comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async></script>
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021-10 ‚Äì 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-star"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Edward PEI</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="Á´ôÁÇπÊÄªÂ≠óÊï∞">57k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Á´ôÁÇπÈòÖËØªÊó∂Èïø">52 ÂàÜÈíü</span>
  </span>
</div>
  <div class="addthis_inline_share_toolbox">
    <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-61691b26da2427b2" async="async"></script>
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>


<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




<script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.7/lib/darkmode-js.min.js"></script>

<script>
var options = {
  bottom: '64px',
  right: 'unset',
  left: '32px',
  time: '0.5s',
  mixColor: 'transparent',
  backgroundColor: 'transparent',
  buttonColorDark: '#100f2c',
  buttonColorLight: '#fff',
  saveInCookies: true,
  label: 'üåì',
  autoMatchOsTheme: true
}
const darkmode = new Darkmode(options);
window.darkmode = darkmode;
darkmode.showWidget();
</script>

  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-needmoreshare2@1/needsharebutton.min.js"></script>
  <script>
      pbOptions = {};
        pbOptions.iconStyle = "box";
        pbOptions.boxForm = "horizontal";
        pbOptions.position = "bottomCenter";
        pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      new needShareButton('#needsharebutton-postbottom', pbOptions);
  </script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"superSample":2,"width":200,"height":400,"position":"right","hOffset":-30,"vOffset":-20},"mobile":{"show":true,"scale":0.5},"react":{"opacityDefault":0.7,"opacityOnHover":0.2},"log":false});</script></body>
</html>
